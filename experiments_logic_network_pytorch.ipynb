{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## XOR perceptron\n",
        "\n",
        "Creating a neural network to solve the XOR function is a classic problem in machine learning, showcasing how non-linear problems can be solved with neural networks.\n"
      ],
      "metadata": {
        "id": "Qbal4UcmFGS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "hmh7H9zLFXmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define \"perceptron\"\n",
        "# a network that takes two input 'units' and produces a single output 'unit'\n",
        "perceptron = nn.Linear(2, 1)\n",
        "perceptron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQGGSgXCFrxs",
        "outputId": "9b10adb7-597a-4d50-ffa8-bb9186a7bfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=2, out_features=1, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "?nn.Linear"
      ],
      "metadata": {
        "id": "4M88e307GHL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = list(perceptron.parameters())  # returns weights and biases\n",
        "perceptron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FD6j-r6F3uO",
        "outputId": "3b9cb067-673a-4786-edce-b2eb09cac55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.1602, -0.6388]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.0024], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have as many weights as we have input units\n",
        "print('Perceptron Weights : ', perceptron[0][0].data.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpkQF3AVF88a",
        "outputId": "2f706dd9-aa88-434c-9c16-113b2f118a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Weights :  [ 0.16023423 -0.638827  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: How many bias terms would we have if we double the size of the input units?\n",
        "print('Perceptron Bias :', perceptron[1].data.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YeR3NEXGDLW",
        "outputId": "03c7da8e-ffa1-4766-9308-0f3c3ac1b199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Bias : [-0.00239065]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input data"
      ],
      "metadata": {
        "id": "aEryt7MQFJas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# create data\n",
        "# start creating the the table for XOR, broken down in Xs and y\n",
        "Xs = torch.Tensor([[0., 0.],\n",
        "                   [0., 1.],\n",
        "                   [1., 0.],\n",
        "                   [1., 1.]])\n",
        "\n",
        "y = torch.Tensor([__, __, __, _]).reshape(Xs.shape[0], 1)\n",
        "# Q: do you remember the tables for y = AND, y = OR?\n",
        "#   how would you represent them?"
      ],
      "metadata": {
        "id": "B6QpFQkG0jj7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5ef8e607-f89d-414a-a70b-265c4aec77eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1fee4aa3df84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                    [1., 1.]])\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# Q: do you remember the tables for y = AND, y = OR?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#   how would you represent them?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not Linear"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![xor.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "Zuu-OWGfAYFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out this this distinction, sometimes we do need singleton dimensions\n",
        "# this is a mild-annoyance from PyTorch's syntax\n",
        "torch.Tensor([1., 1., 1., 1.]).shape, y.shape"
      ],
      "metadata": {
        "id": "1IzpK6s2C2Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xs"
      ],
      "metadata": {
        "id": "bRUA8U6O03bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "qAKT5Vtl06HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?nn.Sigmoid()"
      ],
      "metadata": {
        "id": "z1iSBGeO7T2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "??nn.ReLU()"
      ],
      "metadata": {
        "id": "Rz0TpRuO8dMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Multilayer Perceptron to Compute XOR, AND, OR"
      ],
      "metadata": {
        "id": "QmuhPx-wFbac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pubYyZL2z1iS"
      },
      "outputs": [],
      "source": [
        "# LOGIC is a subclass of nn.Module\n",
        "class LOGIC(nn.Module):\n",
        "    def __init__(self):\n",
        "        # here we define the network architecture\n",
        "        super(LOGIC, self).__init__()\n",
        "        #nn.Linear is a fully connected layer\n",
        "        self.linear = nn.Linear(2, 2)\n",
        "        # nn.Sigmoid is a sigmoid activation\n",
        "        # Q: which activations functions can we use?\n",
        "        self.activation_function = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(2, 1)\n",
        "    # here we define the forward pass\n",
        "    def forward(self, input):\n",
        "        x = self.linear(input)\n",
        "        activation = self.activation_function(x)\n",
        "        # Q: what happens if we skip the sigmoid (or ReLU)? Try feeding x to the next function.\n",
        "        yh = self.linear2(activation)\n",
        "        # Q: what happens if we return activation instead of yh?\n",
        "        return __"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logic_network = LOGIC()\n",
        "# Q: what is an epoch?\n",
        "epochs = 1000\n",
        "mseloss = nn.MSELoss()\n",
        "# The optimizer will perform gradient descent using the network's weights\n",
        "# and a given learning rate\n",
        "optimizer = torch.optim.Adam(logic_network.parameters(), lr = 0.03)\n",
        "all_losses = []\n",
        "current_loss = 0\n",
        "print_every = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # input training example and return the prediction\n",
        "    yhat = logic_network.forward(Xs)\n",
        "\n",
        "    # calculate Mean Squared Error loss of our prediction\n",
        "    loss = mseloss(yhat, y)\n",
        "\n",
        "    # backpropagate to obtain all gradients in the weight layers\n",
        "    loss.backward()\n",
        "\n",
        "    # update model weights\n",
        "    optimizer.step()\n",
        "\n",
        "    import pdb; pdb.set_trace()\n",
        "    # remove current gradients for next iteration\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # append to loss\n",
        "    current_loss += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        all_losses.append(current_loss / print_every)\n",
        "        current_loss = 0\n",
        "\n",
        "    # print progress\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'Epoch: {epoch} completed')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WRPZeqTvz9nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_losses"
      ],
      "metadata": {
        "id": "Xjfs0rkb1Kv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test input\n",
        "input = torch.tensor([0., 0.])\n",
        "out = logic_network(input)\n",
        "out.round()"
      ],
      "metadata": {
        "id": "CiaApGmg61pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([1., 1.])\n",
        "out = logic_network(input)\n",
        "out.round()"
      ],
      "metadata": {
        "id": "YPZwAo408BGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([0., 1.])\n",
        "out = logic_network(input)\n",
        "out.round()"
      ],
      "metadata": {
        "id": "XQbMBOUl8H0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([1., 0.])\n",
        "out = logic_network(input)\n",
        "out.round()"
      ],
      "metadata": {
        "id": "J3LK72Ns8P5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad():\n",
        "  plt.plot(all_losses)\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "wVE-kes_0U6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a80LKIfN1CXA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}